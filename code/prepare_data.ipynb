{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_challenge_dict = json.load(open('../data/RAW_DATA_DIR/arc-prize-2024/arc-agi_training_challenges.json'))\n",
    "training_solutions_dict = json.load(open('../data/RAW_DATA_DIR/arc-prize-2024/arc-agi_training_solutions.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_output_pairs(training_challenge_dict, training_solutions_dict):\n",
    "    input_output_pairs = []\n",
    "    \n",
    "    for challenge_id, challenge in training_challenge_dict.items():\n",
    "        train = challenge['train']\n",
    "        test = challenge['test']\n",
    "\n",
    "        # find how many examples there are in the train\n",
    "        num_train_examples = len(train)\n",
    "\n",
    "        for i in range(num_train_examples):\n",
    "            input_output_pairs.append({\n",
    "                'input': train[i]['input'],\n",
    "                'output': train[i]['output'],\n",
    "                'challenge_id': challenge_id,\n",
    "            })\n",
    "\n",
    "        test_input = test[0]['input']\n",
    "        test_output = training_solutions_dict[challenge_id][0]\n",
    "        input_output_pairs.append({\n",
    "            'input': test_input,\n",
    "            'output': test_output,\n",
    "            'challenge_id': challenge_id,\n",
    "        })\n",
    "        \n",
    "    return input_output_pairs\n",
    "\n",
    "input_output_pairs = create_input_output_pairs(training_challenge_dict, training_solutions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(training_challenge_dict, training_solutions_dict):\n",
    "    dict = {\n",
    "        'input': [],\n",
    "        'output': [],\n",
    "        'challenge_id': []\n",
    "    }\n",
    "\n",
    "    for challenge_id, challenge in training_challenge_dict.items():\n",
    "        train = challenge['train']\n",
    "        test = challenge['test']\n",
    "\n",
    "        for i in range(len(train)):\n",
    "            dict['input'].append(torch.tensor(train[i]['input'], dtype=torch.float32).unsqueeze(0).unsqueeze(0))\n",
    "            dict['output'].append(torch.tensor(train[i]['output'], dtype=torch.float32).unsqueeze(0).unsqueeze(0))\n",
    "            dict['challenge_id'].append(challenge_id)\n",
    "\n",
    "        dict['input'].append(torch.tensor(test[0]['input'], dtype=torch.float32).unsqueeze(0).unsqueeze(0))\n",
    "        dict['output'].append(torch.tensor(training_solutions_dict[challenge_id][0], dtype=torch.float32).unsqueeze(0).unsqueeze(0))\n",
    "        dict['challenge_id'].append(challenge_id)\n",
    "\n",
    "    return pd.DataFrame(dict)\n",
    "\n",
    "dataset = create_dataset(training_challenge_dict, training_solutions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        k1, p1 = 3, 1\n",
    "        k2, p2 = 3, 1\n",
    "        k3, p3 = 3, 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=k1, padding=p1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=k2, padding=p2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=k3, padding=p3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Calculate the size of the feature maps after convolutions and pooling\n",
    "        h1 = input_dim[2] + 2*p1 - (k1-1)\n",
    "        w1 = input_dim[3] + 2*p1 - (k1-1)\n",
    "        h2 = h1 + 2*p2 - (k2-1)\n",
    "        w2 = w1 + 2*p2 - (k2-1)\n",
    "        h3 = h2 + 2*p3 - (k3-1)\n",
    "        w3 = w2 + 2*p3 - (k3-1)\n",
    "\n",
    "        # calculate the output size of the conv layers\n",
    "        conv_output_size = h3 * w3 * 64\n",
    "        \n",
    "        self.fc1 = nn.Linear(conv_output_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim[2] * output_dim[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        #print('x after conv1', x.shape)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        #print('x after conv2', x.shape)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        #print('x after conv3', x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print('x after view', x.shape)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        #print('x after fc1', x.shape)\n",
    "        x = self.fc2(x)\n",
    "        #print('x after fc2', x.shape)\n",
    "        return x.view(-1, self.output_dim[2], self.output_dim[3])\n",
    "    \n",
    "def train(model, inputs, outputs, num_epochs=300, learning_rate=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(inputs)):\n",
    "            input_tensor = inputs[i]\n",
    "            output_tensor = outputs[i]\n",
    "            \n",
    "            # Forward pass\n",
    "            prediction = model(input_tensor)\n",
    "            loss = criterion(prediction, output_tensor)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(inputs)\n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to make predictions\n",
    "def predict(model, input_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    output = output.squeeze().numpy()\n",
    "\n",
    "    # round the output to the nearest integer\n",
    "    output = np.round(output).astype(int)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescobraicovich/Documents/ARC/arcenv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1, 1, 9, 9])) that is different to the input size (torch.Size([1, 9, 9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/300], Average Loss: 0.3458\n",
      "Epoch [50/300], Average Loss: 0.0408\n",
      "Epoch [75/300], Average Loss: 0.0293\n",
      "Epoch [100/300], Average Loss: 0.0372\n",
      "Epoch [125/300], Average Loss: 0.0863\n",
      "Epoch [150/300], Average Loss: 0.0834\n",
      "Epoch [175/300], Average Loss: 0.0461\n",
      "Epoch [200/300], Average Loss: 0.0614\n",
      "Epoch [225/300], Average Loss: 0.0770\n",
      "Epoch [250/300], Average Loss: 0.0388\n",
      "Epoch [275/300], Average Loss: 0.0697\n",
      "Epoch [300/300], Average Loss: 0.0628\n",
      "prediction [[0 0 0 0 6 6 0 6 6]\n",
      " [0 0 0 6 6 6 6 6 6]\n",
      " [0 0 0 0 6 6 0 6 6]\n",
      " [0 6 6 0 6 6 0 6 6]\n",
      " [6 7 6 6 6 6 6 6 6]\n",
      " [0 6 6 0 6 6 0 6 6]\n",
      " [0 0 0 0 6 6 0 6 6]\n",
      " [0 0 0 6 6 6 6 6 6]\n",
      " [0 0 0 0 6 6 0 6 6]]\n",
      "output [[0 0 0 0 7 7 0 7 7]\n",
      " [0 0 0 7 7 7 7 7 7]\n",
      " [0 0 0 0 7 7 0 7 7]\n",
      " [0 7 7 0 7 7 0 7 7]\n",
      " [7 7 7 7 7 7 7 7 7]\n",
      " [0 7 7 0 7 7 0 7 7]\n",
      " [0 0 0 0 7 7 0 7 7]\n",
      " [0 0 0 7 7 7 7 7 7]\n",
      " [0 0 0 0 7 7 0 7 7]]\n",
      "prediction [[7 0 7 0 0 0 7 0 7]\n",
      " [7 0 7 0 0 0 7 0 7]\n",
      " [7 7 0 0 0 0 7 7 0]\n",
      " [7 0 7 0 0 0 7 0 7]\n",
      " [7 0 7 0 0 0 7 0 7]\n",
      " [7 7 0 0 0 0 7 7 0]\n",
      " [7 0 7 7 0 7 0 0 0]\n",
      " [7 0 7 7 0 7 0 0 0]\n",
      " [7 7 0 7 6 0 0 0 0]]\n",
      "output [[7 0 7 0 0 0 7 0 7]\n",
      " [7 0 7 0 0 0 7 0 7]\n",
      " [7 7 0 0 0 0 7 7 0]\n",
      " [7 0 7 0 0 0 7 0 7]\n",
      " [7 0 7 0 0 0 7 0 7]\n",
      " [7 7 0 0 0 0 7 7 0]\n",
      " [7 0 7 7 0 7 0 0 0]\n",
      " [7 0 7 7 0 7 0 0 0]\n",
      " [7 7 0 7 7 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/prwtql5x5sg0mbkxc6508fqm0000gn/T/ipykernel_19280/380552675.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
      "/Users/francescobraicovich/Documents/ARC/arcenv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1, 1, 10, 10])) that is different to the input size (torch.Size([1, 10, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x25600 and 6400x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     k \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN(inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 20\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)):\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# predict the output\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[124], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, inputs, outputs, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     54\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m outputs[i]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(prediction, output_tensor)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ARC/arcenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ARC/arcenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[124], line 40\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#print('x after view', x.shape)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#print('x after fc1', x.shape)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/Documents/ARC/arcenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ARC/arcenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ARC/arcenv/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x25600 and 6400x128)"
     ]
    }
   ],
   "source": [
    "trained_models = {}\n",
    "\n",
    "# unpack the items of the dataset in a loop\n",
    "i = 0\n",
    "while i < len(dataset):\n",
    "\n",
    "    inputs = [dataset.iloc[i]['input']]\n",
    "    outputs = [dataset.iloc[i]['output']]\n",
    "\n",
    "    challenge_id = dataset.iloc[i]['challenge_id']\n",
    "\n",
    "    k = i+1\n",
    "    \n",
    "    while k < len(dataset) and dataset.iloc[k]['challenge_id'] == challenge_id:\n",
    "        inputs.append(dataset.iloc[k]['input'])\n",
    "        outputs.append(dataset.iloc[k]['output'])\n",
    "        k += 1\n",
    "\n",
    "    model = CNN(inputs[0].shape, outputs[0].shape)\n",
    "    trained_model = train(model, inputs, outputs)\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    for j in range(len(inputs)):\n",
    "\n",
    "        # predict the output\n",
    "        prediction = predict(trained_model, inputs[j])\n",
    "\n",
    "        correct_output = outputs[j].squeeze().numpy().astype(int)\n",
    "\n",
    "        if np.array_equal(prediction, correct_output):\n",
    "            correct += 1\n",
    "\n",
    "        else:\n",
    "            print('prediction', prediction)\n",
    "            print('output', correct_output)\n",
    "\n",
    "    # Extract the weights of the fc2 layer\n",
    "    fc2_weights = trained_model.fc2.weight.data\n",
    "    fc2_weights_numpy = fc2_weights.cpu().numpy()  # Convert to NumPy array if needed\n",
    "\n",
    "    trained_models[challenge_id] = {\n",
    "        'fc2_weights': fc2_weights_numpy,\n",
    "        'accuracy': correct/len(inputs),\n",
    "        'correct': correct,\n",
    "        'total': len(inputs),\n",
    "    }\n",
    "\n",
    "    i = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'007bbfb7': {'model': CNN(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (fc1): Linear(in_features=576, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=81, bias=True)\n",
       "  ),\n",
       "  'fc2_weights': array([[ 0.04028171,  0.1018967 ,  0.00197448, ..., -0.02740249,\n",
       "          -0.02646479, -0.00860251],\n",
       "         [-0.06980078,  0.09571728,  0.00734971, ..., -0.05292496,\n",
       "           0.04024561,  0.06641899],\n",
       "         [-0.07581151, -0.01930696,  0.03905515, ..., -0.03314939,\n",
       "           0.02402643, -0.06632405],\n",
       "         ...,\n",
       "         [-0.07378314, -0.0701132 ,  0.00507139, ...,  0.03246288,\n",
       "           0.02805118, -0.04377035],\n",
       "         [ 0.02790444,  0.04738973, -0.06812587, ..., -0.05374504,\n",
       "           0.06247919,  0.04100264],\n",
       "         [ 0.03808356,  0.02758955,  0.00414695, ..., -0.07077436,\n",
       "          -0.00904152,  0.02196288]], dtype=float32),\n",
       "  'accuracy': 1.0,\n",
       "  'correct': 5,\n",
       "  'total': 5}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
